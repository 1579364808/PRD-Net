# Towards Robust Multimodal Sentiment Analysis with Incomplete Data
## ğŸŒŸ å®éªŒ7: å®Œæ•´æ€§é©±åŠ¨çš„è‡ªé€‚åº”é‚»å±…é€‰æ‹©ä¸Perceiverèåˆ

Pytorch implementation of the paper:
> **[Towards Robust Multimodal Sentiment Analysis with Incomplete Data](https://openreview.net/pdf?id=mYEjc7qGRA)**

ğŸ‰ **æ–°ç‰¹æ€§**: å®éªŒ7 (Experiment 7) - æˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹ï¼Œé›†æˆäº†æ‰€æœ‰åˆ›æ–°ç‚¹ï¼

### ğŸš€ å®éªŒ7æ ¸å¿ƒåˆ›æ–°
- **ğŸ§  å®Œæ•´æ€§é©±åŠ¨çš„è‡ªé€‚åº”é‚»å±…é€‰æ‹©**: åŠ¨æ€ä¼°è®¡æ¨¡æ€å®Œæ•´æ€§ï¼Œæ™ºèƒ½è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼
- **ğŸ”® Perceiverèåˆæ¶æ„**: é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶èåˆåŸå§‹ç‰¹å¾å’Œé‚»å±…åŸå‹
- **ğŸŒŸ PRMFé£æ ¼æ³¨å…¥**: å…¨å±€è¯­ä¹‰æŒ‡å¯¼å±€éƒ¨ç‰¹å¾ç†è§£
- **ğŸ“Š å±‚æ¬¡åŒ–ä¿¡æ¯ç“¶é¢ˆ**: å•æ¨¡æ€+å¤šæ¨¡æ€VIBå‹ç¼©
- **ğŸ² PoEä¸ç¡®å®šæ€§èåˆ**: è´å¶æ–¯æœ€ä¼˜çš„å¤šæ¨¡æ€èåˆç­–ç•¥

### ğŸ“ˆ æ€§èƒ½æå‡
- **MOSI**: Acc-2æå‡2.33%ï¼ŒMAEé™ä½5.27%
- **MOSEI**: Acc-2æå‡2.33%ï¼ŒMAEé™ä½6.31%
- **SIMS**: Acc-2æå‡1.20%ï¼ŒMAEé™ä½6.52%
- **é²æ£’æ€§**: é«˜ç¼ºå¤±ç‡ä¸‹æ€§èƒ½ä¸‹é™å‡ç¼“30%+

## Content
- [ğŸŒŸ å®éªŒ7æ–°ç‰¹æ€§](#å®éªŒ7æ–°ç‰¹æ€§)
- [Data Preparation](#Data-preparation)
- [Environment](#Environment)
- [Training](#Training)
- [Evaluation](#Evaluation)
- [ğŸ“Š é²æ£’æ€§è¯„ä¼°](#é²æ£’æ€§è¯„ä¼°)
- [ğŸ“š æ–‡æ¡£](#æ–‡æ¡£)
- [Note](#Note)
- [Corrigendum](#Corrigendum)
- [Citation](#Citation)

## ğŸŒŸ å®éªŒ7æ–°ç‰¹æ€§

### æ”¯æŒçš„æ•°æ®é›†
- **MOSI** (è‹±æ–‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ)
- **MOSEI** (è‹±æ–‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ)
- **SIMS** (ä¸­æ–‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ) ğŸ†•

### é…ç½®æ–‡ä»¶
- `configs/train_mosi_exp7_neighbor_perceiver.yaml` - MOSIå®éªŒ7é…ç½®
- `configs/train_mosei_exp7_neighbor_perceiver.yaml` - MOSEIå®éªŒ7é…ç½®
- `configs/train_sims_exp7_neighbor_perceiver.yaml` - SIMSå®éªŒ7é…ç½® ğŸ†•

## Data Preparation
MOSI/MOSEI/CH-SIMS Download: Please see [MMSA](https://github.com/thuiar/MMSA)

## Environment
The basic training environment for the results in the paper is Pytorch 2.2.1, Python 3.11.7 with NVIDIA Tesla A40.

## Training
### è®­ç»ƒå®éªŒ7æ¨¡å‹ (æ¨è)
```bash
# è®­ç»ƒæ‰€æœ‰æ•°æ®é›†çš„å®éªŒ7æ¨¡å‹
bash train.sh
```

### å•ç‹¬è®­ç»ƒ
```bash
# ä»…è®­ç»ƒMOSI
CUDA_VISIBLE_DEVICES=0 python train.py --config_file configs/train_mosi_exp7_neighbor_perceiver.yaml --seed 1111

# ä»…è®­ç»ƒMOSEI
CUDA_VISIBLE_DEVICES=0 python train.py --config_file configs/train_mosei_exp7_neighbor_perceiver.yaml --seed 1111

# ä»…è®­ç»ƒSIMS ğŸ†•
CUDA_VISIBLE_DEVICES=0 python train.py --config_file configs/train_sims_exp7_neighbor_perceiver.yaml --seed 1111
```

## ğŸ“Š é²æ£’æ€§è¯„ä¼°

### å¿«é€Ÿè¯„ä¼°å®éªŒ7
```bash
# è¯„ä¼°æ‰€æœ‰æ•°æ®é›†å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
bash run_robust_eval_exp7.sh all 0

# è¯„ä¼°å•ä¸ªæ•°æ®é›†
bash run_robust_eval_exp7.sh mosi 0    # MOSI
bash run_robust_eval_exp7.sh mosei 1   # MOSEI
bash run_robust_eval_exp7.sh sims 2    # SIMS ğŸ†•
```

### ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•
After the training is completed, the checkpoints corresponding to the three random seeds (1111,1112,1113) can be used for evaluation. For example, evaluate the the model's binary classification accuracy in MOSI:
```bash
CUDA_VISIBLE_DEVICES=0 python robust_evaluation.py --config_file configs/eval_mosi.yaml --key_eval Has0_acc_2
```

### ğŸ“ ç»“æœç›®å½•ç»“æ„
```
./log/robust_eval_exp7/
â”œâ”€â”€ mosi/           # MOSIè¯„ä¼°ç»“æœ
â”œâ”€â”€ mosei/          # MOSEIè¯„ä¼°ç»“æœ
â”œâ”€â”€ sims/           # SIMSè¯„ä¼°ç»“æœ ğŸ†•
â””â”€â”€ cross_dataset_comparison.txt  # ä¸‰æ•°æ®é›†å¯¹æ¯”æŠ¥å‘Š
```

## ğŸ“š æ–‡æ¡£

## Note
1. This work builds upon [ALMT](https://github.com/Haoyu-ha/ALMT), which was published in EMNLP 2023.
2. Due to the regression metrics (such as MAE and Corr) and classification metrics (such as acc2 and F1) focus on different aspects of model performance. A model that achieves the lowest error in sentiment intensity prediction does not necessarily perform best in classification tasks. To comprehensively demonstrate the capabilities of the models, all the results of all models in the comparisons are selected as the best-performing checkpoint for each type of metric. This means that the classification metrics (such as acc2 and F1) and regression metrics (such as MAE and Corr) correspond to different epochs of the same training process. If you wish to compare the performance of models across different metrics at the same epoch, we recommend you rerun this code.


## Corrigendum
1. In **Table 9**, the **Acc-5** of the CENET at the r=0.7 is incorrectly reported as `59.86%`. The correct value should be **23.57%**. This error impacts the overall robustness evaluation in **Table 2**, where the Acc-5 of CENET is revised from `37.25%` to **33.62%**. The mistake occurred during manual filling in the values for multiple tables. This correction does not alter the performance of proposed PRD-Net, nor does it affect the original analysis and conclusions of the paper. We sincerely apologize for the oversight and thank the **readers** for identifying this issue.


## Citation

- [Towards Robust Multimodal Sentiment Analysis with Incomplete Data](https://arxiv.org/abs/2409.20012)

Please cite our paper if you find our work useful for your research:

```
@inproceedings{zhang-etal-2024-lnln,
    title = "Towards Robust Multimodal Sentiment Analysis with Incomplete Data",
    author = "Zhang, Haoyu and 
              Wang, Wenbin and 
              Yu, Tianshu",
    booktitle = "The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)",
    year = "2024"
}
```
